mc <- cbind(x1,x2)
mr
mc
m3 <- matrix(10 : 19, 2) #10개 데이터를 2행으로 생성
# 자료와 객체 Type 보기
mode(m3); class(m3) #numeric, matrix
# ma행렬 데이터 접근 : [첨자,첨자] 이용
m3[1,] # 1행 전체
m3[,5] # 5열 전체
m3[2,3] #2행 3열의 데이터 1개 -> 15
m3[1, c(2:5)] # 1행에서 2~5열 데이터 4개
d <- c(1:12) # 12개 벡터 객체 생성
arr <- array(d, c(3,2,2)) # 3행2열 구조 2개 -> 배열 객체 생성
arr[,,1] #1면
array
help(array)
arr[,,1] #1면
arr[,,2] #2면
mode(arr); class(arr) #numeric, array
list <- list("lee","이순신",95,"hong","홍길동",85)
unlist <- unlist(list) # list 구조 제거
list <- list("lee","이순신",95,"hong","홍길동",85)
list
member <- list(name="hong",age = 35,
address="한양",gender="남자")
member
num <- list(c(1:5), c(6:10))
num
num2 <- list(first=c(1:5), second=c(6:10))
num2
list <- list("lee","이순신",95,"hong","홍길동",85)
list <- list("lee","이순신",95,"hong","홍길동",85)
list
unlist <- unlist(list) # list 구조 제거
num2 <- list(first=c(1:5), second=c(6:10))
num2
list
list
unlist <- unlist(list) # list 구조 제거
unlist
num <- list(c(1:5), c(6:10))
num
member <- list(name="hong",age = 35,
address="한양",gender="남자")
member
# key, value 형태로 저장
member <- list(name="hong",age = 35,
address="한양",gender="남자")
member
# list 원소 접근 - 변수$키
member$age <- 45
member$age <- 45
member$id <- "hong"
member$pwd <- "1234"
member
member
member <- list(name="hong",age = 35,
address="한양",gender="남자")
member
member$age <- 45
member$id <- "hong"
member$pwd <- "1234"
member
member$age <- NULL  # 원소 제거
member
mode(list); class(list) # list, list
length(member) # 6 -> 리스트 수
a <- list(c(1:5))
b <- list(6:10)
lapply(c(a,b), max) # list로 결과 반환
sapply(c(a,b), max)  # vactor로 결과 반환
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava) # 로딩
library(KoNLP)
library(tm)
library(wordcloud) # RColorBrewer()함수 제공
# [실습] KoNLP에서 제공하는 명사 추출 함수
##################################################
# 단계1 - 토픽분석(텍스트 마이닝)
#- 시각화 : 단어 빈도수에 따른 워드 클라우드
###################################################
# 1) abstract.txt 가져오기
data = read.csv("C:/Rwork/Part-II/abstract.txt",
# stringsAsFactors=FALSE : string을 범주로 사용하지 않음
str(data) # data.frame : 300 obs. of  4 variables: - 행/열 데이터 (관찰치 300개, 변수 4개)
# 실습파일 : abstract.txt
# - 경제학 관련 논문 300개 요약(abstract) 추출-저널,년도,초록
header=TRUE, stringsAsFactors=FALSE)
# 2) Corpus : 벡터 대상 자료집(documents)생성 함수, tm 패키지 제공
result.txt <- Corpus(VectorSource(data[1:100,4]))
# 4번째 컬럼(abstract)만 100개 추출하여 corpus(자료집) 생성
result.txt
inspect(result.txt) #자료집 보기
# 3) 분석 대상 자료집을 대상으로 NA 처리(공백)
result.txt[is.na(result.txt)]   <- ""
result.txt
# 4) 세종 사전 사용 및 단어 추가
useSejongDic() # 세종 사전 불러오기
# 세종 사전에 없는 단어 추가
mergeUserDic(data.frame(c("소셜기업","클라우드펀딩","글로벌경제"), c("ncn")))
# ncn -명사지시코드
# 5) 단어추출 사용자 함수 정의
# (1) 사용자 정의 함수 실행 순서 : 문자변환 -> 명사 단어추출 -> 공백으로 합침
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
# (2) exNouns 함수 이용 단어 추출
# 형식) sapply(적용 데이터, 적용함수) -> 요약 100개를 대상으로 단어 추출
result_nouns <- sapply(result.txt, exNouns)
# (3) 단어 추출 결과
result_nouns[1] # 1번째 백터 요소 보기
# 6) 데이터 전처리
# 추출된 단어 이용 자료집 생성
myCorpus <- Corpus(VectorSource(result_nouns))
myCorpus # Content:  documents: 100
myCorpus <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpus <- tm_map(myCorpus, removeNumbers) # 수치 제거
myCorpus <- tm_map(myCorpus, tolower) # 소문자 변경
myCorpus <-tm_map(myCorpus, removeWords, stopwords('english')) # 불용어제거
# Stopwords 대상 : for, very, and, of, are 등
inspect(myCorpus[1:5]) # 데이터 전처리 결과 확인
# 7) 단어 선별(단어 길이 2개 이상)
# PlainTextDocument 함수를 이용하여 myCorpus를 일반문서로 변경
myCorpus<-tm_map(myCorpus, PlainTextDocument)
myCorpus
# TermDocumentMatrix() : 일반텍스트문서를 대상으로 단어 선별
# 단어길이 2개 이상인 단어만 선별 -> matrix 변경
myTerm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(2,Inf)))
myTerm
# matrix -> data.frame 변경
myTerm.df <- as.data.frame(as.matrix(myTerm))
myTerm.df
dim(myTerm.df) # [1] 4850  100
# 8) 단어 빈도수 구하기
wordResult <- sort(rowSums(myTerm.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:5]
wordResult
# wordcloud 생성 (디자인 적용전)
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
#myName
# 9) 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# 단어이름과 빈도수로 data.frame 생성
x11( ) # 별도의 창을 띄우는 함수
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
# 색상 지정(2개 중 하나 - 워드 색상 지정)
pal <- brewer.pal(12,"Paired")
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# 색상, 빈도수, 글꼴, 회전 등 적용
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
#wordcloud(단어, 빈도수, 5:1비율 크기,최소빈도수,랜덤순서,랜덤색상, 회전비율, 색상(파렛트),컬러,글꼴 )
# 10) 차트 시각화
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
names(topWord)
# 키워드와 백분율 적용
lab <- paste(names(topWord), "\n", pct, "%")
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava) # 로딩
# 3) install.packages
install.packages(c("KoNLP", "tm", "wordcloud"))
# 4) 패키지 로딩
library(KoNLP)
install.packages(c("KoNLP", "tm", "wordcloud"))
library(tm)
library(wordcloud) # RColorBrewer()함수 제공
data = read.csv("C:/Rwork/Part-II/abstract.txt",
header=TRUE, stringsAsFactors=FALSE)
# stringsAsFactors=FALSE : string을 범주로 사용하지 않음
str(data) # data.frame : 300 obs. of  4 variables: - 행/열 데이터 (관찰치 300개, 변수 4개)
# 실습파일 : abstract.txt
# - 경제학 관련 논문 300개 요약(abstract) 추출-저널,년도,초록
# 2) Corpus : 벡터 대상 자료집(documents)생성 함수, tm 패키지 제공
result.txt <- Corpus(VectorSource(data[1:100,4]))
# 4번째 컬럼(abstract)만 100개 추출하여 corpus(자료집) 생성
result.txt
inspect(result.txt) #자료집 보기
# 3) 분석 대상 자료집을 대상으로 NA 처리(공백)
result.txt[is.na(result.txt)]   <- ""
result.txt
# 4) 세종 사전 사용 및 단어 추가
useSejongDic() # 세종 사전 불러오기
# 세종 사전에 없는 단어 추가
mergeUserDic(data.frame(c("소셜기업","클라우드펀딩","글로벌경제"), c("ncn")))
# ncn -명사지시코드
# 5) 단어추출 사용자 함수 정의
# (1) 사용자 정의 함수 실행 순서 : 문자변환 -> 명사 단어추출 -> 공백으로 합침
exNouns <- function(x) { paste(extractNoun(as.character(x)), collapse=" ")}
# (2) exNouns 함수 이용 단어 추출
# 형식) sapply(적용 데이터, 적용함수) -> 요약 100개를 대상으로 단어 추출
result_nouns <- sapply(result.txt, exNouns)
# (3) 단어 추출 결과
result_nouns[1] # 1번째 백터 요소 보기
# 6) 데이터 전처리
# 추출된 단어 이용 자료집 생성
myCorpus <- Corpus(VectorSource(result_nouns))
myCorpus # Content:  documents: 100
myCorpus <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpus <- tm_map(myCorpus, removeNumbers) # 수치 제거
myCorpus <- tm_map(myCorpus, tolower) # 소문자 변경
myCorpus <-tm_map(myCorpus, removeWords, stopwords('english')) # 불용어제거
# Stopwords 대상 : for, very, and, of, are 등
inspect(myCorpus[1:5]) # 데이터 전처리 결과 확인
# 7) 단어 선별(단어 길이 2개 이상)
# PlainTextDocument 함수를 이용하여 myCorpus를 일반문서로 변경
myCorpus<-tm_map(myCorpus, PlainTextDocument)
myCorpus
# TermDocumentMatrix() : 일반텍스트문서를 대상으로 단어 선별
# 단어길이 2개 이상인 단어만 선별 -> matrix 변경
myTerm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(2,Inf)))
myTerm
# matrix -> data.frame 변경
myTerm.df <- as.data.frame(as.matrix(myTerm))
myTerm.df
dim(myTerm.df) # [1] 4850  100
# 8) 단어 빈도수 구하기
wordResult <- sort(rowSums(myTerm.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:5]
wordResult
# wordcloud 생성 (디자인 적용전)
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
#myName
x11( ) # 별도의 창을 띄우는 함수
# 9) 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
# 색상 지정(2개 중 하나 - 워드 색상 지정)
pal <- brewer.pal(12,"Paired")
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# 색상, 빈도수, 글꼴, 회전 등 적용
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
# 10) 차트 시각화
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
names(topWord)
# 키워드와 백분율 적용
lab <- paste(names(topWord), "\n", pct, "%")
# 파이차트에 제목과 백분율 적용
pie(topWord, main="기업경영 논문집  토픽분석",
col=rainbow(10), cex=0.8, labels=lab)
# 파이차트 위에 공백 원형 추가
par(new=T)
pie(word, radius=0.6, col="white", labels=NA, border=NA)
par(new=T)
pie(word, radius=0.6, col="white", labels=NA, border=NA)
par(new=T)
pie(word, radius=0.6, col="white", labels=NA, border=NA)
pie(topWord, main="기업경영 논문집  토픽분석",
col=rainbow(10), cex=0.8, labels=lab)
# 파이차트 위에 공백 원형 추가
par(new=T)
pie(word, radius=0.6, col="white", labels=NA, border=NA)
par(new=T)
pie(word.df, radius=0.6, col="white", labels=NA, border=NA)
par(new=T)
pie(topWord, radius=0.6, col="white", labels=NA, border=NA)
x11( ) # 별도의 창을 띄우는 함수
pie(topWord, main="기업경영 논문집  토픽분석",
col=rainbow(10), cex=0.8, labels=lab)
# 파이차트 위에 공백 원형 추가
par(new=T)
pie(topWord, radius=0.6, col="white", labels=NA, border=NA)
library(ggmap)
# google 지도 불러오기
map <- get_map("Jeonju", zoom=14, maptype='satellite', scale=2)
# 형식) get_map("중심지역", 확대비율, 지도유형)
ggmap(map, size=c(600,600), extent='device') # 장치 허용크기로 표시
pop <- read.csv("C:/Rwork/Part-III/population2014.csv",header=T)
pop
lon <- pop$LON
lat <- pop$LAT
data <- pop$총인구수
# 데이터프레임 생성
df <- data.frame(lon,lat,data)
df
# 지도정보 생성
map <- get_map("Jeonju", zoom=7 , maptype='roadmap')
# 레이어1: 지도 플로팅
map1 <- ggmap(map)
map1
# 레이어2 : 포인트 추가
map1 + geom_point(aes(x=lon,y=lat,colour=data,size=data),data=df)
# 크기, 넓이, 폭, 밀도 적용 파일 저장
ggsave("C:/Rwork/output/pop.png",scale=1,width=7,height=4,dpi=1000)
# 다양한 지도 유형
install.packages("rJava")
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_60')
library(rJava) # 로딩
library(xlsx) # 로딩
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_60')
library(rJava) # 로딩
library(xlsx) # 로딩
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_51')
library(rJava) # 로딩
library(xlsx) # 로딩
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_51')
library(rJava) # 로딩
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_51')
library(rJava) # 로딩
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_60')
library(rJava) # 로딩
library(xlsx) # 로딩
library(lattice)
library(mlmRev)
histogram(~gcsescore, data=Chem97)
densityplot(~gcsescore | factor(score), data=Chem97,
groups = gender, plot.points=T, auto.key = T)
xyplot(lat~long, data=quakes, pch=".")
# 그래프를 변수에 저장
tplot<-xyplot(lat~long, data=quakes, pch=".")
# 그래프에 제목 추가
tplot2<-update(tplot,
main="1964년 이후 태평양에서 발생한 지진위치")
print(tplot2)
cloud(depth ~ lat * long , data=quakes,
zlim=rev(range(quakes$depth)),
xlab="경도", ylab="위도", zlab="깊이")
library(maps) #  map()함수
library(mapproj) # projection 지원
# maps 패키지에서 제공되는 map()함수
# map database을 이용하여 특별한 선이나 다각형을 그려준다.
# database와 투사방법에 의해서 지도가 그려진다.
# 형식) map(database, plot, fill, projection)
map = map("county", plot = F, fill = T, projection = "mercator")
# 형식)  mapplot(y축 ~ x축 , data, map)
mapplot(rownames(USCancerRates) ~ log(rate.male) + log(rate.female),
data = USCancerRates, map)
library(maps) #  map()함수
library(mapproj) # projection 지원
map = map("county", plot = F, fill = T, projection = "mercator")
mapplot(rownames(USCancerRates) ~ log(rate.male) + log(rate.female),
data = USCancerRates, map)
library(latticeExtra)
mapplot(rownames(USCancerRates) ~ log(rate.male) + log(rate.female),
data = USCancerRates, map)
library(DBI)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_60')
library(rJava)
library(RJDBC) # rJava에 의존적이다.(rJava 먼저 로딩)
install.packages(c("KoNLP", "tm", "wordcloud"))
#[실습]-------------------------------------------
myNoun <- extractNoun("내 이름은 홍길동 입니다.")
myNoun
paste(myNoun, collapse = " ") # 공백으로 단어 연결
library(KoNLP) # 한글처리 - Sejong(),extractNoun() 함수 제공
library(tm)  # 텍스트 마이닝 - Corpus()함수 제공
library(wordcloud) # 단어 구름 - RColorBrewer()함수 제공
#[실습]-------------------------------------------
myNoun <- extractNoun("내 이름은 홍길동 입니다.")
myNoun
paste(myNoun, collapse = " ") # 공백으로 단어 연결
data = read.csv("C:/Rwork/Part-II/abstract.txt",
header=TRUE, stringsAsFactors=FALSE)
# stringsAsFactors=FALSE : string을 범주로 사용하지 않음
head(data,1)# 1행 보기
str(data) # data.frame : 300 obs. of  4 variables:
result.txt <- Corpus(VectorSource(data[1:100,4]))
# 4번째 컬럼(abstract)만 100개 추출하여 corpus(자료집) 생성
result.txt # Content:  documents: 100
inspect(result.txt) #자료집 내용 보기- Content:  chars: 546
# 3) 분석 대상 자료집을 대상으로 NA 처리(공백)
result.txt[is.na(result.txt)]   <- ""
result.txt
# 4) 세종 사전 사용 및 단어 추가
useSejongDic() # 세종 사전 불러오기-KoNLP 제공
#Backup was just finished!
#87007 words were added to dic_user.txt
# 세종 사전에 없는 단어 추가
mergeUserDic(data.frame(c("소셜기업","클라우드펀딩","글로벌경제"), c("ncn")))
# ncn -명사지시코드
# 5) 단어추출 사용자 정의 함수
#  실행 순서 : 문자변환 -> 명사 단어추출 -> 공백으로 합침
exNouns <- function(x) {
paste(extractNoun(as.character(x)), collapse=" ")
}
#[실습]-------exNouns 함수 테스트 -------
text <- "나는 대한민국 사람 입니다."
exNouns(text) # [1] "나 대한 민국 사람"
#----------------------------------------
# (1) exNouns 함수 이용 단어 추출
# 형식) sapply(적용 데이터, 적용함수) : list 처리 함수
result_nouns <- sapply(result.txt, exNouns) # result.txt : list type
result_nouns[1] # 1번째 백터 요소 보기
# 6) 데이터 전처리
# 추출 단어 대상으로 자료집 다시 생성(why : 추출과정에 type 변경됨)
myCorpus <- Corpus(VectorSource(result_nouns))
myCorpus
myCorpus <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpus <- tm_map(myCorpus, removeNumbers) # 수치 제거
myCorpus <- tm_map(myCorpus, tolower) # 소문자 변경
myCorpus <-tm_map(myCorpus, removeWords, stopwords('english')) # 불용어제거
# Stopwords 대상 : for, very, and, of, are 등
inspect(myCorpus[1:5]) # 데이터 전처리 결과 확인
myText <-tm_map(myCorpus, PlainTextDocument)
myText
myTerm <- TermDocumentMatrix(myText, control=list(wordLengths=c(2,Inf)))
myTerm # <<TermDocumentMatrix (terms: 4850, documents: 100)>>
# myTerm : 완전한 matrix형이 아님
myTerm.df <- as.data.frame(as.matrix(myTerm))
dim(myTerm.df) # [1] 4850  100
# 8) 단어 빈도수 구하기
wordResult <- sort(rowSums(myTerm.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:5] # 1~ 5번째 빈도수
#경영 전략 투자 자금 효과
#327  263  154  150  121
# 9) wordcloud 생성 (디자인 적용전)
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적성
x11( ) # 별도의 창을 띄우는 함수
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
# 색상 지정(2개 중 하나 - 워드 색상 지정)
#pal <- brewer.pal(9,"Set1") # Set1~ Set3
pal <- brewer.pal(12,"Paired")
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# 색상, 빈도수, 글꼴, 회전 등 적용
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
# 11) 차트 시각화
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
names(topWord)
# 키워드와 백분율 적용
lab <- paste(names(topWord), "\n", pct, "%")
pie(topWord, main="기업경영 논문집  토픽분석",
col=rainbow(10), cex=0.8, labels=lab)
par(new=T)
pie(topWord, radius=0.6, col="white", labels=NA, border=NA)
install.packages("ggmap") # ‘ggmap’와 ‘ggplot2’(우선 설치) 관련 패키지
library(ggplot2)
library(ggmap)
gc <- geocode("seoul, korea") # geolocation API 이용
center <- as.numeric(gc)
center # 위도,경도
library(ggmap)
library(ggplot2)
gc <- geocode("seoul, korea") # geolocation API 이용
gc <- geocode("seoul, korea") # geolocation API 이용
library(ggmap)
library(ggplot2)
gc <- geocode("seoul, korea") # geolocation API 이용
gc <- geocode("seoul) # geolocation API 이용
gc <- geocode("seoul") # geolocation API 이용
geocode("seoul, korea", source=c("dsk","googole"))
geocode("seoul, korea", source="googole")
geocode("seoul, korea", source="google")
geocode("seoul, korea", source=c("dsk","google"))
center <- as.numeric(gc)
center # 위도,경도
gc<- geocode("seoul, korea", source="google")# geolocation API 이용
center <- as.numeric(gc)
center # 위도,경도
gc <- geocode("waco, texas", source="google") # texas주 중부 도시
center <- as.numeric(gc)
center # 위도,경도
setwd("c:/Rwork/Part-III")
data <- read.csv("one_sample.csv", header=TRUE)
head(data)
x <- data$survey
# 2. 빈도수와 비율 계산
summary(x) # 결측치 확인
length(x) # 150개
table(x) # 0:불만족(14), 1: 만족(136)
library(prettyR) # freq() 함수 사용
freq(x)
freq(x)
binom.test(c(136,14), p=0.8) # 기존 80% 만족율 기준 검증 실시
binom.test(c(136,14), p=0.8, alternative="two.sided", conf.level=0.95)
# 단측검정
binom.test(c(136,14), p=0.8, alternative="greater", conf.level=0.95)
binom.test(c(136,14), p=0.8, alternative="two.sided", conf.level=0.95)
binom.test(c(136,14), p=0.8, alternative="two.sided", conf.level=0.95)
binom.test(c(14,136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
binom.test(c(14,136), p=0.2, alternative="two.sided", conf.level=0.95)
binom.test(c(14,136), p=0.2, alternative="greater", conf.level=0.95)
setwd("c:/Rwrok/Part-III")
data <- read.csv("one_sample.csv", header=TRUE)
str(data) # 150
head(data)
x <- data$time
head(x)
# 2. 기술통계량 평균 계산
summary(x) # NA-41개
mean(x) # NA
mean(x, na.rm=T) # NA 제외 평균(방법1)
x1 <- na.omit(x) # NA 제외 평균(방법2)
mean(x1)
shapiro.test(x1) # 정규분포 검정 함수(p-value = 0.7242)
shapiro.test(x1) # 정규분포 검정 함수(p-value = 0.7242)
# 4. 가설검정 - 모수/비모수
# 정규분포(모수검정) -> t.test()
# 비정규분포(비모수검정) -> wilcox.test()
# 1) 양측검정 - 정제 데이터와 5.2시간 비교
t.test(x1, mu=5.2)
t.test(x1, mu=5.2, alter="two.side", conf.level=0.95) # p-value = 0.0001417
# 해설 : 평균 사용시간 5.2시간과 차이가 있다.
# 2) 단측검정
t.test(x1, mu=5.2, alter="greater", conf.level=0.95)
